---
layout: post
title: dataX
categories:  大数据平台开发
tags: [数据交换工具]［大数据平台工具］
fullview: true
---

# dataX

### dataX简介
* dataX是一个在异构的数据库／文件系统之间高速交换的工具，实现了在任意的数据处理系统（RDBMS／HDFS/HDFS/local filesystem)之间的数据交换，由淘宝数据平台部门完成。

### dataX用来解决什么问题
* 目前成熟的数据导入导出工具比较多(jdbcdump/dbloader/multithread/getmerge+sqlloader/mysqldumper…).但是一般都只能用于数据导入或者导出，并且只能支持一个或者几个特定类型的数据库。
* 这些工具有些使用文件中转数据，有些使用管道，不同程度的为数据中转带来额外开销，效率差别很非常大。
* 很多工具也无法满足ETL任务中常见的需求，比如日期格式转化，特性字符的转化，编码转换。
* 有些时候，我们希望在一个很短的时间窗口内，将一份数据从一个数据库同时导出到多个不同类型的数据库。

### dataX的特点
* 在异构的数据库／文件系统之间高速交换数据
* 采用Framework+plugin架构构建，Framework处理了缓冲，流控，并发，上下文加载等高速数据交换的大部分技术问题，提供了简单的接口与插件交互，插件仅需实现对数据处理系统的访问
* 运行模式：stand-alone
* 数据传输过程在单进程内完成，全内存操作，不读写磁盘，也没有IPC
* 开放式的框架，开发者可以在极短的时间开发一个新插件以快速支持新的数据库/文件系统

### dataX架构模式
<img src="/img/DataX_structure.jpg"/>

* Job: 一道数据同步作业
* Splitter: 作业切分模块，将一个大任务与分解成多个可以并发的小任务.
Sub-job： 数据同步作业切分后的小任务
* Reader(Loader): 数据读入模块，负责运行切分后的小任务，将数据从源头装载入DataX
* Storage: Reader和Writer通过Storage交换数据
* Writer(Dumper): 数据写出模块，负责将数据从DataX导入至目的数据地

DataX框架内部通过双缓冲队列、线程池封装等技术，集中处理了高速数据交换遇到的问题，提供简单的接口与插件交互，插件分为Reader和Writer两类，基于框架提供的插件接口，可以十分便捷的开发出需要的插件。比如想要从oracle导出数据到mysql，那么需要做的就是开发出OracleReader和MysqlWriter插件，装配到框架上即可。并且这样的插件一般情况下在其他数据交换场合是可以通用的。更大的惊喜是我们已经开发了如下插件：

###### Reader插件
* hdfsreader : 支持从hdfs文件系统获取数据。
* mysqlreader: 支持从mysql数据库获取数据。
* sqlserverreader: 支持从sqlserver数据库获取数据。
* oraclereader : 支持从oracle数据库获取数据。
* streamreader: 支持从stream流获取数据（常用于测试）
* httpreader : 支持从http URL获取数据。

###### Writer插件
* hdfswriter：支持向hdbf写入数据。
* mysqlwriter：支持向mysql写入数据。
* oraclewriter：支持向oracle写入数据。
* streamwriter：支持向stream流写入数据。（常用于测试）

##命令行使用
* 配置
  1. 执行/home/taobao/datax/bin/datax.py –e true 命令
  2. 选择数据源端对应的数字
  3. 选择数据宿端对应的数字
  4. 到相应的目录下去编辑配置生成的 job xml 即可。(自劢生成的 xml 文件中,有“?” 标识的 value 值,表示此处用户必须配置,其他地方的默认值用户可以根据自己需要 作修改)。
* 运行
  1. 任务启动命令 /home/taobao/datax/bin/datax.py job.xml
  2. 运行时添加参数定义
