---
layout: post
title: dataX
categories:  大数据
tags: [数据交换工具]
fullview: true
---

# dataX

### dataX简介
* dataX是一个在异构的数据库／文件系统之间高速交换的工具，实现了在任意的数据处理系统（RDBMS／HDFS/HDFS/local filesystem)之间的数据交换，由淘宝数据平台部门完成。

### dataX用来解决什么问题
* 目前成熟的数据导入导出工具比较多(jdbcdump/dbloader/multithread/getmerge+sqlloader/mysqldumper…).但是一般都只能用于数据导入或者导出，并且只能支持一个或者几个特定类型的数据库。
* 这些工具有些使用文件中转数据，有些使用管道，不同程度的为数据中转带来额外开销，效率差别很非常大。
* 很多工具也无法满足ETL任务中常见的需求，比如日期格式转化，特性字符的转化，编码转换。
* 有些时候，我们希望在一个很短的时间窗口内，将一份数据从一个数据库同时导出到多个不同类型的数据库。

### dataX的特点
* 在异构的数据库／文件系统之间高速交换数据
* 采用Framework+plugin架构构建，Framework处理了缓冲，流控，并发，上下文加载等高速数据交换的大部分技术问题，提供了简单的接口与插件交互，插件仅需实现对数据处理系统的访问
* 运行模式：stand-alone
* 数据传输过程在单进程内完成，全内存操作，不读写磁盘，也没有IPC
* 开放式的框架，开发者可以在极短的时间开发一个新插件以快速支持新的数据库/文件系统

### dataX架构模式
<img src="/img/DataX_structure.jpg"/>

* Job: 一道数据同步作业
* Splitter: 作业切分模块，将一个大任务与分解成多个可以并发的小任务.
Sub-job： 数据同步作业切分后的小任务
* Reader(Loader): 数据读入模块，负责运行切分后的小任务，将数据从源头装载入DataX
* Storage: Reader和Writer通过Storage交换数据
* Writer(Dumper): 数据写出模块，负责将数据从DataX导入至目的数据地

DataX框架内部通过双缓冲队列、线程池封装等技术，集中处理了高速数据交换遇到的问题，提供简单的接口与插件交互，插件分为Reader和Writer两类，基于框架提供的插件接口，可以十分便捷的开发出需要的插件。比如想要从oracle导出数据到mysql，那么需要做的就是开发出OracleReader和MysqlWriter插件，装配到框架上即可。并且这样的插件一般情况下在其他数据交换场合是可以通用的。更大的惊喜是我们已经开发了如下插件：

###### Reader插件
* hdfsreader : 支持从hdfs文件系统获取数据。
* mysqlreader: 支持从mysql数据库获取数据。
* sqlserverreader: 支持从sqlserver数据库获取数据。
* oraclereader : 支持从oracle数据库获取数据。
* streamreader: 支持从stream流获取数据（常用于测试）
* httpreader : 支持从http URL获取数据。

###### Writer插件
* hdfswriter：支持向hdbf写入数据。
* mysqlwriter：支持向mysql写入数据。
* oraclewriter：支持向oracle写入数据。
* streamwriter：支持向stream流写入数据。（常用于测试）

##命令行使用
* 配置
  1. 执行/home/taobao/datax/bin/datax.py –e true 命令
  2. 选择数据源端对应的数字
  3. 选择数据宿端对应的数字
  4. 到相应的目录下去编辑配置生成的 job xml 即可。(自劢生成的 xml 文件中,有“?” 标识的 value 值,表示此处用户必须配置,其他地方的默认值用户可以根据自己需要 作修改)。
* 运行
  1. 任务启动命令 /home/taobao/datax/bin/datax.py job.xml
  2. 运行时添加参数定义

## dataX数据导入
1.  写json上传到hdfs上
2.  通过Oozie ssh 85上
3.  hadoop get json文件
4.  datax.py 执行它

```
python /opt/datax/bin/datax.py /opt/datax/sjfxz/jyyc.json
sh down_script_run_datax.sh /user/tangjie/jyyc.json /opt/datax/blw_datax
```

credit_code就是统一社会信用代码


## 大数据平台数据从hive导出到db2
全量的方式

1. 创建当前目录下的这次导入的脚本目录,创建相应的表名对应的shell脚本
2. 将这些脚本考到各个集群中的相应目录下
3. 执行相应的脚本将调用hql查询出来的数据保存到/root/data_db2_日期/表名.txt中
4. 再将/root/data_db2_日期文件夹下的所有的文件拷到db2instl这台机器的/home/db2instl/data_final/下
5. 删除/root/data_db2_日期
6. 在用db2将txt数据导入


每次数据过来导入到ods中通过diff找出有改变的数据或者新增的数据，将所有有改变的数据的唯一id和修改模式保存到一张变化表中，根据这张表的内容生成相应的增删改查sql，db2调用这些sql就能实现增量更新。<br>

hive的transform关键字提供了在sql中调用自写脚本的功能，适合实现Hive中没有的功能又不想写udf的情况<br>

hive输出的时候直接输出sql，db2直接执行这些sql完成增量更新<br>
