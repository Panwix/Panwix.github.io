---
layout: post
title: 大数据平台搭建
categories:  大数据平台
tags: 大数据平台
fullview: true
---

# 大数据开发环境搭建

## hadoop
###### 1. 创建用户hadoop，密码和用户名相同。这个用户用于实际工作，root用户仅用于配置环境
		useradd hadoop
		passwd hadoop
###### 2. 主机命名和注册
首先修改每台主机的/etc/hostname文件，确定主机名，然后修改每台主机的/etc/hosts文件，将主机名和ip地址对应关系写进去
###### 3. 开启不要密码的ssh登录
hadoop对其他DataNode的操作都用的是ssh连接，频繁的操作肯定不能不停的输入密码。理论上每个设备都应该创建密钥并在其他设备上注册，但其实在NameNode上也可以，因为一般情况下都是NameNode操作DataNode。

	＃创建用户密钥，root用户和hadoop用户都要创建
	ssh-keygen -t rsa

	#将密钥在DataNode上注册，注意要切换用户
	ssh-copy-id root@Master.hadoop
	ssh-copy-id hadoop@Master.hadoop

###### 4.java运行环境
在/etc/profile中：

		 export JAVA_HOME=/usr/lib/jvm/java-openjdk
 		 export JRE_HOME=/usr/lib/jvm/java-openjdk/jre
 		 export CLASSPATH=.:${CLASSPATH}:${JAVA_HOME}/lib:${JRE_HOME}/lib
 		 export PATH=${PATH}:${JAVA_HOME}/bin:${JRE_HOME}/bin
###### 5.配置hadoop
把hadoop交给hadoop用户

		```bash
		cd /usr
		tar zxvf hadoop-2.6.0.tar.gz
		mv hadoop-2.6.0 hadoop
		chown -R hadoop:hadoop hadoop
		``` 		 
1. 先在 hadoop 安装目录中创建一个 tmp 目录，未来的查询都可以在这个目录下进行

		cd /usr/hadoop
 		mkdir tmp		
2. 将 hadoop 需要的环境变量加入 /etc/profile 文件中去，这个步骤需要 root 权限
在文件最后加上下面的内容，然后拷贝到其他主机中去：

		# Set hadoop environment
 		export HADOOP_HOME=/usr/hadoop
 		export PATH=$PATH:$HADOOP_HOME/bin

3. 配置三个xml文件：
		* core-site.xml
		* hdfs-site.xml
		* mapred-site.xml

		其中 core-site.xml 和 hdfs-site.xml 是站在 HDFS 角度上配置文件； core-site.xml 		和 mapred-site.xml 是站在 MapReduce 角度上配置文件。

	1. 配置 hadoop-env.sh `export JAVA_HOME=/usr/lib/jvm/java-openjdk`
	2. 配置 core-site.xml

			 <configuration>
     			<property>
         			<name>hadoop.tmp.dir</name>
         			<value>/usr/hadoop/tmp</value>
         			<description>A base for other temporary directories.
         			</description>
     			</property>

     			<property>
         			<name>fs.defaultFS</name>
         			<value>hdfs://Master.hadoop</value>
     			</property>

     			<!-- file system properties -->
     			<property>
         			<name>fs.default.name</name>
         			<value>hdfs://Master.hadoop:9000</value>
     			</property>

 			</configuration>

 	3. 配置hdfs-site.xml

 		 	<configuration>
     			<property>
         		<name>dfs.replication</name>
         		<value>2</value>
     			</property>
 			</configuration>

 	4. 配置 mapred-site.xml</br>
 		hadoop 2.6 不需要配置这个文件		

 	5. 主机配置
 		1. 修改 /usr/hadoop/etc/hadoop/masters 文件，默认如果没有那么就得新建一个，在里面加			上一行：
 			`192.168.110.11 或者名称也可以 Master.hadoop`
 		2. 修改 /usr/hadoop/etc/hadoop/slaves 文件，在里面把其他 DataNode 加进来
 			`Slave_1.hadoop`</br>
			`Slave_2.hadoop`

	6. slave 机器配置

			```bash
			scp -r /usr/hadoop root@192.168.110.12:/usr
			chown -R hadoop:hadoop /usr/hadoop
			```		 		
			`scp /etc/profile root@192.168.110.12:/etc`

	7. 格式化HDFS系统
		`hadoop namenode -format`
	8. 启动hadoop
		```bash
		./sbin/start-dfs.sh
		./sbin/start-yarn.sh
		```		

### Hive
###### 1. 源文件
下载、解压hive到/usr/hadoop/thirdparty
###### 2. 设置 Hive环境变量
编辑 /etc/profile文件, 在其中添加以下内容：

		# Hive Env
		export HIVE_HOME=/usr/hadoop-2.6.4/thirdparty/apache-hive-2.1.0-bin
		export PATH=$PATH:$HIVE_HOME/bin

###### 3. 配置hive环境变量
		cd /usr/hadoop-2.6.4/thirdparty/apache-hive-2.1.0-bin/conf
		cp hive-env.sh.template hive-env.sh

		cp hive-default.xml.template hive-site.xml

		cp hive-log4j2.properties.template hive-log4j2.properties

		cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties

###### 4. 修改hive-env.sh
		export JAVA_HOME=/usr/java/jdk-1.8.0_101    ##Java路径
		export HADOOP_HOME=/usr/hadoop-2.6.4   ##Hadoop安装路径
		export HIVE_HOME=/usr/hadoop-2.6.4/thirdparty/apache-hive-2.1.0-bin   ##Hive安装路径
		export HIVE_CONF_DIR=$HIVE_HOME/conf    ##Hive配置文件路径

####### 5.修改hive-site.xml
		<property>
    		<name>hive.exec.scratchdir</name>
    		<value>/tmp/hive-${user.name}</value>
    		<description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>
  		</property>
  		<property>
    		<name>hive.exec.local.scratchdir</name>
    		<value>/tmp/${user.name}</value>
    		<description>Local scratch space for Hive jobs</description>
  		</property>
 		<property>
    		<name>hive.downloaded.resources.dir</name>
    		<value>/tmp/hive/resources</value>
    		<description>Temporary local directory for added resources in the remote file system.</description>
  		</property>
		<property>
    		<name>hive.querylog.location</name>
    		<value>/tmp/${user.name}</value>
    		<description>Location of Hive run time structured log file</description>
  		</property>
		<property>
    		<name>hive.server2.logging.operation.log.location</name>
    		<value>/tmp/${user.name}/operation_logs</value>
    		<description>Top level directory where operation logs are stored if logging functionality is enabled</description>
  		</property>

######### 6. 配置Hive Metastore

默认情况下, Hive的元数据保存在了内嵌的 derby 数据库里, 但一般情况下生产环境使用 MySQL 来存放 Hive 元数据。

将 mysql-connector-java-5.1.40-bin.jar 放入 $HIVE_HOME/lib 下。
hive-site.xml 中配置 MySQL 数据库连接信息。

		<property>
 			<name>javax.jdo.option.ConnectionURL</name>
 			<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
		</property>
		<property>
 			<name>javax.jdo.option.ConnectionDriverName</name>
 			<value>com.mysql.jdbc.Driver</value>
		</property>
		<property>
 			<name>javax.jdo.option.ConnectionUserName</name>
 			<value>hive</value>
		</property>
		<property>
 			<name>javax.jdo.option.ConnectionPassword</name>
 			<value>hive</value>
		</property>  		
######## 7. 为hive创建hdfs目录
在 Hive 中创建表之前需要使用以下 HDFS 命令创建 /tmp 和 /user/hive/warehouse (hive-site.xml 配置文件中属性项 hive.metastore.warehouse.dir 的默认值) 目录并给它们赋写权限。

		start-dfs.sh
		hdfs dfs -mkdir /tmp
		hdfs dfs -mkdir -p /usr/hive/warehouse
		hdfs dfs -chmod g+w /tmp
		hdfs dfs -chmod g+w /usr/hive/warehouse

###### 8. 给mysql创建用户hive／密码hive
		$ mysql -u root -p #密码已设为123456
		mysql> CREATE USER 'hive'@'localhost' IDENTIFIED BY "hive";
		mysql> grant all privileges on *.* to hive@localhost identified by 'hive';

####### 9. 运行hive
		$ schematool -dbType mysql -initSchema
		 hive								
