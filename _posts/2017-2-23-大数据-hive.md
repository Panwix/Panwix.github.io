---
layout: post
title: hive
categories:  大数据平台开发
tags: [数据仓库]
fullview: true
---

# Hive

#### Hive是什么
* Hive是基于Hadoop的一个数据仓库工具，通过他可以实现将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，同时将sql语句转换为MapReduce任务进行运行

#### Hive不是什么
* 一个关系数据库
* 一个设计用于联机事务处理（OLTP）
* 实时查询和行级更新的语言

#### Hive的特点
* 它存储架构在一个数据库中并处理数据到HDFS
* 它专门为OLAP(联机分析处理)设计
* 它提供SQL类型语言查询叫HiveQL或HQL
* 它熟知，快速和可扩展

#### Hive和Hadoop的交互流程
1. Execute Query: Hive接口，如命令行或Web UI发送查询驱动程序（任何数据库驱动程序，如JDBC，ODBC等）来执行
2. Get Plan: 在驱动程序帮助下查询编译器，分析查询检查语法和查询计划或查询的要求
3. Get Metadata: 编译器发送元数据请求到Metastore（任何数据库）。
4. Send Metadata: Metastore发送元数据，以编译器的响应。
5. Send Plan: 编译器检查要求，并重新发送计划给驱动程序。到此为止，查询解析和编译完成。
6. Execute Plan: 驱动程序发送的执行计划到执行引擎。
7. Execute Job: 在内部，执行作业的过程是一个MapReduce工作。执行引擎发送作业给JobTracker，在名称节点并把它分配作业到TaskTracker，这是在数据节点。在这里，查询执行MapReduce工作。
7.1 Metadata Ops:与此同时，在执行时，执行引擎可以通过Metastore执行元数据操作。
8. Fetch Result: 执行引擎接收来自数据节点的结果。
9. Send Results: 执行引擎发送这些结果值给驱动程序。
10. Send Results: 驱动程序将结果发送给Hive接口。

#### DDL操作
* 建表
* 删除表
* 修改表结构
* 创建／删除视图
* 创建数据库
* 显示命令

###### 建外部表
		CREATE EXTERNAL TABLE IF NOT EXISTS mydb.table (
			name string,
			....
		)
		COMMIT 'Description of the table'
		ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054'
		STORED AS TEXTFILE
		LOCATION '/user/hive/warehouse/mydb.db/table'

###### 建分区表
		CREATE TABLE par_table(
			name string,
		)
		COMMENT 'This is the page view table'
		PARTITIONED BY(date STRING)
		ROW FORMAT DELIMITED ‘\t’
		FIELDS TERMINATED BY '\n'
		STORED AS SEQUENCEFILE;

最后的STORED AS 子句，指的是Hive数据文件的存储格式，这里使用的是TEXTFILE，还有SEQUENCEFILE和RCFile，一共三种。
`TEXTFILE`是最普通的文件存储格式，内容是可以直接查看。<br>
`SEQUCENFILE`是包含键值对的二进制的文件存储格式，支持压缩，可以节省存储空间。是Hadoop领域的标准文件格式，但是在hadoop之外却无法使用。<br>
`RCFile`是列式存储文件格式，适合压缩处理。对于有成百上千字段的表而言，RCFile更加合适。

###### 建Bucket表
		CREATE TABLE par_table(
			name string,
			...
		)
		COMMIT 'This is the page view table'
		PARTITIONED BY(date STRING)
		CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
		ROW FORMAT DELIMITED '\t'
		FIELDS TERMINATED BY '\n'
		STORED AS SEQUENCEFILE;

###### 复制一个空表
		CREATE TABLE empty_key_value_store
		LIKE key_value_store;		

###### 数据格式
		row format delimited

		fields terminated by '\t'

 		lines terminated by '\n';
 		字段之间是tab键分割，行之间是断行

###### 修改表结构
* 增加分区、删除分区
* 重命名表
* 修改列的名字、类型、位置、注释
* 增加／更新列
* 增加表的元数据信息

###### 表添加一列
ALTER TABLE pokes ADD COLUMNS (new_col INT);
###### 添加一列并增加列字段注释
ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');
###### 更改表名
ALTER TABLE events RENAME TO 3koobecaf;
###### 删除表
ROP TABLE pokes;
###### 增加、删除分区
* 增加
	* 对于外部表

			alter table empl_ext add partition (logdate=‘2015-02-26’) location ‘hdfs://nameservice1/vod_pb/’;      执行添加分区时   vod_pb/    文件夹下的数据不会被移动。并且没有分区目录logdate=2015-02-26

	* 对于内部表

			alter table empl_inn add partition (logdate=‘2015-02-26’) location ‘hdfs://nameservice1/vod_pb/’;      执行添加分区时   vod_pb/    文件夹下的数据不会被移动。并且没有分区目录logdate=2015-02-26

* 删除
	* 对于外部表

			alter table empl_ext drop partition (logdate=‘2015-02-26’);     执行删除分区目录时vod_pb/    下的数据不会被删除

	* 对于内部表

			 alter table empl_inn drop partition (logdate=‘2015-02-26’);     执行删除分区时vod_pb/    下的数据会被删除并且连同vod_pb/文件夹也会被删除

* 导数据
	* 对于外部表

			load data inpath 'hdfs://nameservice1/vod_pb/' overwrite into table empl_ext   partition(logdate='2015-02-26');    执行加载数据添加分区时 vod_pb/     文件夹下的数据会被移动，并创建分区目录logdate=2015-02-26，数据移动到此目录下

	* 对于内部表

			load data inpath 'hdfs://nameservice1/vod_pb/' overwrite into table empl_inn   partition(logdate='2015-02-26');    执行加载数据添加分区时 vod_pb/     文件夹下的数据会被移动，并创建分区目录logdate=2015-02-26，数据移动到此目录下

###### 重命名表
ALTER TABLE table_name RENAME TO new_table_name

###### 修改列的名字、类型、位置、注释
ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]

###### 表添加一列
ALTER TABLE pokes ADD COLUMNS (new_col INT);

###### 添加一列并增加列字段注释
ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');

###### 增加/更新列
ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)  

###### 增加表的元数据信息

	ALTER TABLE table_name SET TBLPROPERTIES table_properties table_properties:
         :[property_name = property_value…..]

###### 改变表文件格式与组织
* ALTER TABLE table_name SET FILEFORMAT file_format        
* ALTER TABLE table_name CLUSTERED BY(userid) SORTED BY(viewTime) INTO num_buckets BUCKETS

###### 创建／删除表
* CREATE VIEW [IF NOT EXISTS] view_name [ (column_name [COMMENT 	column_comment], ...) ][COMMENT view_comment][TBLPROPERTIES (property_name = 	property_value, ...)] AS SELECT
* 增加视图
* 如果没有提供表名，视图列的名字将由定义的SELECT表达式自动生成
* 如果修改基本表的属性，视图中不会体现，无效查询将会失败
* 视图是只读的，不能用LOAD/INSERT/ALTER
* DROP VIEW view_name
* 删除视图

###### 创建数据库
CREATE DATABASE name

###### 显示命令
* show tables
* show databases
* show partitions
* show functions
* describe extended table_name dot col_name

#### DML 操作:元数据存储
hive不支持用insert语句一条一条的进行插入操作，也不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。

###### 向数据表内夹在文件
* LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
* Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。
* filepath
* 相对路径，例如：project/data1
* 绝对路径，例如： /user/hive/project/data1
* 包含模式的完整 URI，例如：hdfs://namenode:9000/user/hive/project/data1

###### 加载本地数据，同时给定分区信息
* 加载的目标可以是一个表或者分区。如果表包含分区，必须指定每一个分区的分区名
* filepath 可以引用一个文件（这种情况下，Hive 会将文件移动到表所对应的目录中）或者是一个目录（在这种情况下，Hive 会将目录中的所有文件移动至表所对应的目录中）

###### LOCAL关键字
* 指定了LOCAL，即本地
* load 命令会去查找本地文件系统中的 filepath。如果发现是相对路径，则路径会被解释为相对于当前用户的当前路径。用户也可以为本地文件指定一个完整的 URI，比如：file:///user/hive/project/data1.
* load 命令会将 filepath 中的文件复制到目标文件系统中。目标文件系统由表的位置属性决定。被复制的数据文件移动到表的数据对应的位置

###### 将查询结果插入Hive表
* 将查询结果插入Hive表
* 将查询结果写入HDFS文件系统
* 基本模式
	* INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement
* 多插入模式
	* FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...

###### 将查询结果写入HDFS文件系统
* INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...
* 数据写入文件系统时进行文本序列化，且每列用^A 来区分，\n换行
###### 将查询结果写入hive
INSERT INTO  TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement

#### DQL 操作:数据查询SQL
###### 基本的Select 操作

	SELECT [ALL | DISTINCT] select_expr, select_expr, ...
	FROM table_reference
	[WHERE where_condition]
	[GROUP BY col_list [HAVING condition]]
	[   CLUSTER BY col_list
  	| [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
	]
	[LIMIT number]

* 使用ALL和DISTINCT选项区分对重复记录的处理。默认是ALL，表示查询所有记录。DISTINCT表示去掉重复的记录
* Where 条件类似我们传统SQL的where 条件目前支持 AND,OR ,0.9版本支持between,IN, NOT IN,不支持EXIST ,NOT EXIST
* ORDER BY与SORT BY的不同
	* ORDER BY 全局排序，只有一个Reduce任务
	* SORT BY 只在本机做排序
* Limit 可以限制查询的记录数
	* SELECT * FROM t1 LIMIT 5
* 实现Top k 查询
* 下面的查询语句查询销售记录最大的 5 个销售代表。
	* SET mapred.reduce.tasks = 1
  	SELECT * FROM test SORT BY amount DESC LIMIT 5
* REGEX Column Specification
SELECT 语句可以使用正则表达式做列选择，下面的语句查询除了 ds 和 hr 之外的所有列：
SELECT `(ds|hr)?+.+` FROM test

###### 基于Partition的查询
* 一般 SELECT 查询会扫描整个表，使用 PARTITIONED BY 子句建表，查询就可以利用分区剪枝（input pruning）的特性
* Hive 当前的实现是，只有分区断言出现在离 FROM 子句最近的那个WHERE 子句中，才会启用分区剪枝

###### jion
1. 内关联（JOIN）：只返回能关联上的结果。
2. 左外关联（LEFT [OUTER] JOIN）：以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。
3. 右外关联（RIGHT [OUTER] JOIN）：和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。
4. 全外关联（FULL [OUTER] JOIN）：以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。
5. LEFT SEMI JOIN：以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录。
6. 笛卡尔积关联（CROSS JOIN）：返回两个表的笛卡尔积结果，不需要指定关联键。

#### SQL和HiveQL使用不同点
1. Hive不支持等值连接

		SQL中对两表内联可以写成：
		select * from dual a,dual b where a.key = b.key;
		Hive中应为
		select * from dual a join dual b on a.key = b.key;


2. 分号字符

		语句中出现分号需要使用分号的八进制的ASCII码进行转义

3. IS [NOT] NULL

		SQL中null代表空值, 值得警惕的是, 在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False.

4. Hive不支持将数据插入现有的表或分区中 仅支持覆盖重写整个表
5. hive不支持INSERT INTO, UPDATE, DELETE操作
6. hive支持嵌入mapreduce程序，来处理复杂的逻辑
7. hive支持将转换后的数据直接写入不同的表，还能写入分区、hdfs和本地目录。

		FROM t1  

		INSERT OVERWRITE TABLE t2  
		SELECT t3.c2, count(1)  
		FROM t3  
		WHERE t3.c1 <= 20  
		GROUP BY t3.c2  

		INSERT OVERWRITE DIRECTORY '/output_dir'  
		SELECT t3.c2, avg(t3.c1)  
		FROM t3  
		WHERE t3.c1 > 20 AND t3.c1 <= 30  
		GROUP BY t3.c2  

		INSERT OVERWRITE LOCAL DIRECTORY '/home/dir'  
		SELECT t3.c2, sum(t3.c1)  
		FROM t3  
		WHERE t3.c1 > 30  
		GROUP BY t3.c2;  

####  内部表与外部表的区别
Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。
