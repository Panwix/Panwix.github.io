---
layout: post
title: spark
categories:  大数据平台开发
tags: [spark]
fullview: true
---

# Spark学习
### 大数据技术框架
可分为6层:

1. 数据收集（etl、提取、转换、加载）
2. 数据存储（sql、NoSql）
3. 资源管理
4. 计算框架（批处理、交互式分析、流处理）
5. 数据挖掘、分析（数据仓库、olap、商务智能）
6. 数据可视化

spark：计算框架、数据分析


### hadoop与spark生态系统
计算框架：
1. 批处理计算
2. 迭代式与DAG计算
3. 交互式计算
4. 流式计算


离线计算：MR、Hive、Spark、Tez、Shark、Stinger<br>
交互式计算框架：Impala、Presto<br>
流式／实时计算框架：Storm、Spark Streaming、Samza<br>

### 大数据基础
hdfs、yarn、mapreduce

mapreduce局限性：
1. 仅支持map和reduce操作
2. 处理效率低
3. 不适合做复杂计算
4. mapreduce编程不够灵活

#### 框架多样化
将三种框架统一

### Spark特点
1. 高效：内存计算、DAG引擎、使用多线程池模型来减少task启动开销
2. 易用
3. 与hadoop集成

#### RDD
弹性（可以是内存也可以是磁盘）分布式数据集：
1. 分布在集群中的只读对象集合（由多个partition构成与hdfs的block相同）
2. 可以存储在磁盘或内存中
3. 通过并行转换操作
4. 失效后自动重构

RDD计算：
1. Transformation：将数据构造成新的rdd
2. action：通过rdd计算的到一个或多个值


##### spark rdd cache/persist
1. 允许将rdd缓存到内存或磁盘上
2. 提供了多级缓存

##### transformation与action
transformation不会分布式执行，直到遇到一个action


#### spark运行模式
yarn-client:客户端，方便调试
yarn-cluster:高容错

## Spark Core

### Spark系统架构
* 应用程序(Application): 基于spark的用户程序，包含了一个Driver Program和集群中多个的Executor；
* 驱动(Driver):运行Application的main()函数并且创建SparkContext；
* 执行单元(Executor):是为某Appliaction运行在Worker Node上的一个进程，该进程负责运行task，并且负责将数据存在内存或磁盘上，每个Application都有各自独立的Executors；
* 集群管理程序(Cluster Manager):在集群上获取资源的外部服务(local,standalone,mesos或yarn等集群管理系统)
* 操作(Operation): 作用于RDD的各种操作分为Transformation和Action.

```
整个 Spark 集群中,分为 Master 节点与 worker 节点,其中 Master 节点上常驻 Master 守护进程和 Driver 进程, Master 负责将串行任务变成可并行执行的任务集Tasks, 同时还负责出错问题处理等,而 Worker 节点上常驻 Worker 守护进程, Master 节点与 Worker 节点分工不同, Master 负载管理全部的 Worker 节点,而 Worker 节点负责执行任务.
Driver 的功能是创建 SparkContext, 负责执行用户写的 Application 的 main 函数进程,Application 就是用户写的程序.
Spark 支持不同的运行模式,包括Local, Standalone,Mesoses,Yarn 模式.不同的模式可能会将 Driver 调度到不同的节点上执行.集群管理模式里, local 一般用于本地调试.
每个 Worker 上存在一个或多个 Executor 进程,该对象拥有一个线程池,每个线程负责一个 Task 任务的执行.根据 Executor 上 CPU-core 的数量,其每个时间可以并行多个 跟 core 一样数量的 Task
```
### Spark运行原理
1. 使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程
```
根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。
```
2. Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批Task，然后将这些Task分配到各个Executor进程中执行。
```
Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个Task可能都会从上一个stage的Task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。
```
3. 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个Task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。
```
因此Executor的内存主要分为三块：第一块是让Task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让Task通过shuffle过程拉取了上一个stage的Task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。
```
4. Task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个Task，都是以每个Task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的Task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些Task线程。

### 如何运行Spark运行程序
要启动 Spark 运行程序主要有两种方式:一种是使用 spark-submit 将脚本文件提交,一种是打开 Spark 跟某种特定语言的解释器,如:

* spark-shell: 启动了 Spark 的 scala 解释器.
* pyspark: 启动了 Spark 的 python 解释器.
* sparkR: 启动了 Spark 的 R 解释器.

### RDD
RDD(Resilent Distributed Datasets)俗称弹性分布式数据集,是 Spark 底层的分布式存储的数据结构,可以说是 Spark 的核心, Spark API 的所有操作都是基于 RDD 的. 数据不只存储在一台机器上,而是分布在多台机器上,实现数据计算的并行化.弹性表明数据丢失时,可以进行重建.
```
RDD 是一种只读的数据块,可以从外部数据转换而来,你可以对RDD 进行函数操作(Operation),包括 Transformation 和 Action. 在这里只读表示当你对一个 RDD 进行了操作,那么结果将会是一个新的 RDD, 这种情况放在代码里,假设变换前后都是使用同一个变量表示这一 RDD,RDD 里面的数据并不是真实的数据,而是一些元数据信息,记录了该 RDD 是通过哪些 Transformation 得到的,在计算机中使用 lineage 来表示这种血缘结构,lineage 形成一个有向无环图 DAG, 整个计算过程中,将不需要将中间结果落地到 HDFS 进行容错,加入某个节点出错,则只需要通过 lineage 关系重新计算即可.
```
###### 1). RDD 主要具有如下特点:
1. 它是在集群节点上的不可变的、已分区的集合对象;
2. 通过并行转换的方式来创建(如 Map、 filter、join 等);
3. 失败自动重建;
4. 可以控制存储级别(内存、磁盘等)来进行重用;
5. 必须是可序列化的;
6. 静态类型的(只读)。

###### 2). RDD 的创建方式主要有2种:
1. 并行化(Parallelizing)一个已经存在与驱动程序(Driver Program)中的集合如set、list;
2. 读取外部存储系统上的一个数据集，比如HDFS、Hive、Hbase,或者任何提供了Hadoop InputFormat的数据源.也可以从本地读取 txt、csv 等数据集

###### 3). RDD 的操作函数(operation)主要分为2种类型 Transformation 和 Action.
\ 类别        \ 函数    \  区别 \
\ :------:   \ :-----:   \ :----: \
\ Transformation        \ Map,filter,groupBy,join, union,reduce,sort,partitionBy      \   返回值还是 RDD,不会马上 提交 Spark 集群运行    \
\ Action        \ count,collect,take,save, show      \   返回值不是 RDD,会形成 DAG 图,提交 Spark 集群运行 并立即返回结果    \

```
Transformation 操作不是马上提交 Spark 集群执行的,Spark 在遇到 Transformation 操作时只会记录需要这样的操作,并不会去执行,需要等到有 Action 操作的时候才会真正启动计算过程进行计算.针对每个 Action,Spark 会生成一个 Job, 从数据的创建开始,经过 Transformation, 结尾是 Action 操作.这些操作对应形成一个有向无环图(DAG),形成 DAG 的先决条件是最后的函数操作是一个Action.
```

### Spark的stage、job、task
* stage:
```
在一个job 中划分stage 的一个重要依据是否有shuflle 发生 ，也就是是否会发生数据的重组 （重新组织数据）。
在一个stage 内部会有很多的task 被执行，在同一个stage 中 所有的task 结束后才能根据DAG 依赖执行下一个stage 中的task.
job 有很多任务组成，每组任务可以任务是一个stage
```
* task:
```
task 跟 partition  block 等概念紧密相连 ，task 是执行job 的逻辑单元 ，在task 会在每个executor 中的cpu core 中执行
```
* job
```
job 可以认为是我们在driver 或是通过spark-submit 提交的程序中一个action ，在我们的程序中有很多action  所有也就对应很多的jobs
```

### partition 与 block
```
hdfs中的block是分布式存储的最小单元，类似于盛放文件的盒子，一个文件可能要占多个盒子，但一个盒子里的内容只可能来自同一份文件。假设block设置为128M，你的文件是250M，那么这份文件占3个block（128+128+2）。这样的设计虽然会有一部分磁盘空间的浪费，但是整齐的block大小，便于快速找到、读取对应的内容。（p.s. 考虑到hdfs冗余设计，默认三份拷贝，实际上3*3=9个block的物理空间。）spark中的partition 是弹性分布式数据集RDD的最小单元，RDD是由分布在各个节点上的partition 组成的。partition 是指的spark在计算过程中，生成的数据在计算空间内最小单元，同一份数据（RDD）的partition 大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定的，这也是为什么叫“弹性分布式”数据集的原因之一。
```
总结：

* block位于存储空间、partition 位于计算空间，
* block的大小是固定的、partition 大小是不固定的，block是有冗余的、不会轻易丢失，
* partition（RDD）没有冗余设计、丢失之后重新计算得到

### shuffle 与 stage
Shuffle是MapReduce框架中的一个特定的phase,介于Map phase和Reduce phase之间，当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。<br>
shuffle write 将 ShuffleMapTask 任务产生的中间结果缓存到内存中, shuffle fetch 获得 ShuffleMapTask 缓存的中间结果进行 ShuffleReduceTask 计算,这个过程容易造成OutOfMemory<br>
进行 shuffle 操作的是是很消耗系统资源的,需要写入到磁盘并通过网络传输,有时还需要对数据进行排序.<br>
shuffle 过程内存分配使用 ShuffleMemoryManager 类管理,会针对每个 Task 分配内存,Task 任务完成后通过 Executor 释放空间.这里可以把 Task 理解成不同 key 的数据对应一个 Task.

### 调优
* 每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作 ---- 对多次使用的RDD进行持久化
* repartition，join，cogroup，以及任何 *By 或者 *ByKey 的 Transformation 都需要 shuffle 数据,合理的选用操作将降低 shuffle 操作的成本,提高运算速度.

### Spark Streaming
```
Spark Streaming是基于Spark Core基础之上用于处理实时计算业务的框架。其实现就是把输入的流数据进行按时间切分，切分的数据块用离线批处理的方式进行并行计算处理
从整体上看，实时计算与离线计算一样，主要组件是Driver和Executor的，不同的是多了数据采集和数据按时间分片过程，数据采集依赖外部数据源，数据分片则依靠内部一个时钟，按batch interval来定时对数据分片，然后把每一个batch interval内的数据提交处理。
```
构建一个 Spark Streaming 应用程序一般来说需要 4 个步骤：

1. 创建 StreamingContext对象。需要注意的是参数 Second(1)，Spark Streaming 需要制定处理数据的时间间隔，如 1s，那么 Spark Streaming 会以 1s 为时间窗口进行数据处理。
2. 创建 InputDStream如同 Strom 的 Spout 一样，Spark Streaming 需要指明数据源。例如 socketTextStream，Spark Streaming 将以套接字连接作为数据源读取数据。当然，Spark Streaming 支持多种不同的数据源，包括 kafkaStream、flumeStream、fileStream、networkStream等
3. 操作 DStream对于从数据源得到的 DStream，用户可以在其基础上进行各种操作
4. 启动 Spark Streaming之前的所有步骤只创建了执行流程，程序没有有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计算，当 ssc.start() 启动后，程序才真正进行所有预期的操作。

### Spark sql
Spark SQL是Spark的一个组件，用于结构化数据的计算。Spark SQL提供了一个称为DataFrames的编程抽象，DataFrames可以充当分布式SQL查询引擎。
DataFrame是一个分布式的数据集合，该数据集合以命名列的方式进行整合。DataFrame可以理解为关系数据库中的一张表，也可以理解为R/Python中的一个data frame。

### SQLContext
Spark SQL程序的主入口是SQLContext类或它的子类。<br>
`JavaSparkContext sc = ...; // An existing JavaSparkContext.`<br>
`SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);`<br>
除了基本的SQLContext，也可以创建HiveContext。SQLContext和HiveContext区别与联系为：
* SQLContext现在只支持SQL语法解析器（SQL-92语法）
* HiveContext现在支持SQL语法解析器和HiveSQL语法解析器，默认为HiveSQL语法解析器，用户可以通过配置切换成SQL语法解析器，来运行HiveSQL不支持的语法。
* 使用HiveContext可以使用Hive的UDF，读写Hive表数据等Hive操作。SQLContext不可以对Hive进行操

### 创建DataFrames
使用SQLContext，spark应用程序（Application）可以通过RDD、Hive表、JSON格式数据等数据源创建DataFrames。

		JavaSparkContext sc = ...; // An existing JavaSparkContext.
		SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);

		DataFrame df = sqlContext.read().json("examples/src/main/resources/people.json");

		// Displays the content of the DataFrame to stdout
		df.show();

### DataFrame操作
* df.show()
* df.printSchema()
* df.select("name").show()
* df.select(df.col("name"), df.col("age").plus(1)).show();
* df.filter(df.col("age").gt(21)).show();
* df.groupBy("age").count().show();

### 运行SQL查询程序
Spark Application可以使用SQLContext的sql()方法执行SQL查询操作，sql()方法返回的查询结果为DataFrame格式		

```
SQLContext sqlContext = ...  // An existing SQLContext
DataFrame df = sqlContext.sql("SELECT * FROM table")
```

### DataFrames与RDDs的相互转换
Spark SQL支持两种RDDs转换为DataFrames的方式：

1. 使用反射获取RDD内的Schema(当已知类的Schema的时候，使用这种基于反射的方法会让代码更加简洁而且效果也很好。)
2. 通过编程接口指定Schema(通过Spark SQL的接口创建RDD的Schema，这种方式会让代码比较冗长。这种方法的好处是，在运行时才知道数据的列以及列的类型的情况下，可以动态生成Schema)

### 数据源
Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。

###### 一般的load和save方法
Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。

```
DataFrame df = sqlContext.read().load("examples/src/main/resources/users.parquet");
df.select("name", "favorite_color").write().save("namesAndFavColors.parquet");
```

###### 手动指定选项
当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称（json,parquet,jdbc）。通过指定的数据源格式名，可以对DataFrames进行类型转换操作。

```
DataFrame df = sqlContext.read().format("json").load("examples/src/main/resources/people.json");
df.select("name", "age").write().format("parquet").save("namesAndAges.parquet");
```

###### 存储模式
可以采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。
4种模式：

1. ErrorIFExists
2. Append
3. Overwrite
4. ignore

###### 持久化到表

当使用HiveContext时，可以通过saveAsTable方法将DataFrames存储到表中。与registerTempTable方法不同的是，saveAsTable将DataFrame中的内容持久化到表中，并在HiveMetastore中存储元数据。存储一个DataFrame，可以使用SQLContext的table方法。table先创建一个表，方法参数为要创建的表的表名，然后将DataFrame持久化到这个表中。

默认的saveAsTable方法将创建一个“managed table”，表示数据的位置可以通过metastore获得。当存储数据的表被删除时，managed table也将自动删除。

#### Parquet文件
Parquet是一种支持多种数据处理系统的柱状的数据格式，Parquet文件中保留了原始数据的模式。Spark SQL提供了Parquet文件的读写功能。

###### 3.2.2 解析分区信息
对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。

自动解析分区类型的参数为：spark.sql.sources.partitionColumnTypeInference.enabled，默认值为true。

###### Schema合并
像ProtocolBuffer、Avro和Thrift那样，Parquet也支持Schema evolution（Schema演变）。用户可以先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。

因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：

1. 当数据源为Parquet文件时，将数据源选项mergeSchema设置为true
2. 设置全局SQL选项spark.sql.parquet.mergeSchema为true

#### Hive metastore Parquet表转换
当向Hive metastore中读写Parquet表时，Spark SQL将使用Spark SQL自带的Parquet SerDe（SerDe：Serialize/Deserilize的简称,目的是用于序列化和反序列化），而不是用Hive的SerDe，Spark SQL自带的SerDe拥有更好的性能。这个优化的配置参数为spark.sql.hive.convertMetastoreParquet，默认值为开启。

###### Hive/Parquet Schema反射
从表Schema处理的角度对比Hive和Parquet，有两个区别：

1. Hive区分大小写，Parquet不区分大小写
2. hive允许所有的列为空，而Parquet不允许所有的列全为空

当将Hive metastore Parquet表转换为Spark SQL Parquet表时，需要将Hive metastore schema和Parquet schema进行一致化。一致化规则如下：

1. 这两个schema中的同名字段必须具有相同的数据类型。一致化后的字段必须为Parquet的字段类型。这个规则同时也解决了空值的问题。
2. 一致化后的schema只包含Hive metastore中出现的字段

###### 元数据刷新
Spark SQL缓存了Parquet元数据以达到良好的性能。当Hive metastore Parquet表转换为enabled时，表修改后缓存的元数据并不能刷新。所以，当表被Hive或其它工具修改时，则必须手动刷新元数据，以保证元数据的一致性。
`sqlContext.refreshTable("my_table")`

#### 配置
配置Parquet可以使用SQLContext的setConf方法或使用SQL执行SET key=value命令。

### JSON数据集
Spark SQL能自动解析JSON数据集的Schema，读取JSON数据集为DataFrame格式。读取JSON数据集方法为SQLContext.read().json()。该方法将String格式的RDD或JSON文件转换为DataFrame。

需要注意的是，这里的JSON文件不是常规的JSON格式。JSON文件每一行必须包含一个独立的、自满足有效的JSON对象。如果用多行描述一个JSON对象，会导致读取出错。

### Hive表
```
// sc is an existing JavaSparkContext.
HiveContext sqlContext = new org.apache.spark.sql.hive.HiveContext(sc.sc);

sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)");
sqlContext.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src");

// Queries are expressed in HiveQL.
Row[] results = sqlContext.sql("FROM src SELECT key, value").collect();
```

### JDBC To Other Databases
Spark SQL支持使用JDBC访问其他数据库。当时用JDBC访问其它数据库时，最好使用JdbcRDD。使用JdbcRDD时，Spark SQL操作返回的DataFrame会很方便，也会很方便的添加其他数据源数据。JDBC数据源因为不需要用户提供ClassTag，所以很适合使用Java或Python进行操作。
使用JDBC访问数据源，需要在spark classpath添加JDBC driver配置。

```
Map<String, String> options = new HashMap<String, String>();
options.put("url", "jdbc:postgresql:dbserver");
options.put("dbtable", "schema.tablename");

DataFrame jdbcDF = sqlContext.read().format("jdbc"). options(options).load();
```

### Spark SQL支持的Hive特性
Spark SQL支持多部分的Hive特性，例如：

* Hive查询语句，包括：
	* SELECT
	* GROUP BY
	* ORDER BY
	* CLUSTER BY
	* SORT BY
* 所有Hive运算符，包括
	* 比较操作符（=, ⇔, ==, <>, <, >, >=, <=, etc）
	* 算术运算符（+, -, *, /, %, etc）
	* 逻辑运算符（AND, &&, OR, ||, etc）
	* 复杂类型构造器
	* 数学函数（sign,ln,cos,etc）
	* 字符串函数（instr,length,printf,etc）
* 用户自定义函数（UDF）
* 用户自定义聚合函数（UDAF）
* 用户自定义序列化格式器（SerDes）
* 窗口函数
* Joins
	* JOIN
	* {LEFT/RIGHT/FULL} OUTER JOIN
	* LEFT SEMI JOIN
	* CROSS JOIN
* Unions
* 子查询
	* SELECT col FROM ( SELECT a + b AS col from t1) t2
* Sampling
* Explain
* 表分区，包括动态分区插入
* 视图
* 所有的Hive DDL函数，包括：
	* CREATE TABLE
	* CREATE TABLE AS SELECT
	* ALTER TABLE
* 大部分的Hive数据类型，包括：
	* TINYINT
	* SMALLINT
	* INT
	* BIGINT
	* BOOLEAN
	* FLOAT
	* DOUBLE
	* STRING
	* BINARY
	* TIMESTAMP
	* DATE
	* ARRAY<>
	* MAP<>
	* STRUCT<>

### 不支持的Hive功能

* Tables with buckets：bucket是在一个Hive表分区内进行hash分区。
* UNION type
* Unique join
* Column statistics collecting
* File format for CLI
* Hadoop archive
